---
title: "ArchaeoGLOBE trend analysis"
author: "Nick Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document: 
    highlight: pygments
    latex_engine: lualatex
  html_document: 
    highlight: pygments
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      cache = FALSE)
```
Sample analysis code for the ArchaeoGlobe database. Here we use Generalized Additive Models (GAMs), a flexible form of nonlinear regression model capable of fitting smooth, time-varying trends to the ordered categorical ArchaeoGLOBE response data.

We model ordered categorical data using a latent variable following a logistic distribution. The model identifies a series of cut points, which correspond the the probabilities of the latent variable falling within each of our categories.

We fit two sets of trends. One trend is fitted to all the data simultaneously, representing the global trend across all archaeological regions. Then we fit region-level trends, which represent the deviation of each region from the global trend. By penalizing the "wiggliness" of the trend lines, we allow regional trends that don't significantly deviate from the global trend to be penalized to 0, effectively reducing that particular region to the global trend. This is a form of partial pooling, allowing the model to share information between groups and in so doing make the results less sensitive to regions with exceptionally low response rates.

After fitting the model, we can extract the region-specific deviations from the global trend, use a k-means clustering alogirithm to group together regions with similar trends, and map the results. We repeat this analysis for both self-reported expertise and perceived data quality.

# Setup

Import packages needed for analysis. We'll use packages from the `tidyverse`, such as `readr`, `dplyr`, and `ggplot2` for data import, processing, and plotting. We'll also use `mgcv` for fitting nonlinear trends to the data. We'll use the `sf` package to help us plot shapefiles in a tidy context. Finally, we'll use `patchwork` to combine multiple ggplots in the same image.

```{r, message = FALSE}
library(tidyverse)
library(mgcv)
library(sf)
library(ggplot2)

#install patchwork from github 
#devtools::install_github('thomasp85/patchwork')
library(patchwork)
```

## Data import

Read in the latest version of the ArchaeoGLOBE database and the regions' shapefile from the Dataverse repository.

```{r dataverse-database, echo=T}
library("dataverse")
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

# get data frame of files on dataverse
ArchaeoGLOBE_Public_Data_DOI <- 
  "doi:10.7910/DVN/CNCANQ"
ArchaeoGLOBE_Public_Data_df <- 
  get_dataset(ArchaeoGLOBE_Public_Data_DOI)

# Only download the file we need here
ArchaeoGLOBE_Public_Data_df_files <- 
  ArchaeoGLOBE_Public_Data_df$files[grepl("ARCHAEOGLOBE_PUBLIC_DATA|ARCHAEOGLOBE_CONSENSUS_ASSESSMENT", 
                                          ArchaeoGLOBE_Public_Data_df$files$filename), ]

# read into local dir
walk(ArchaeoGLOBE_Public_Data_df_files$label,
     ~get_file(.x, ArchaeoGLOBE_Public_Data_DOI) %>% 
       writeBin(paste0('data/raw-data/', .x)))

# read into the current environment
archaeoglobe <- read_csv('data/raw-data/ARCHAEOGLOBE_PUBLIC_DATA.tab')
consensus <- read_csv('data/raw-data/ARCHAEOGLOBE_CONSENSUS_ASSESSMENT.tab')
```

```{r dataverse-shapefile, echo=T}
# repeat for shapefile
ArchaeoGLOBE_Regions_DOI <-  
  "doi:10.7910/DVN/CQWUBI"

# get data frame of files on DV
ArchaeoGLOBE_Regions_df <- 
  get_dataset(ArchaeoGLOBE_Regions_DOI) 

# just download the shapefile we want
ArchaeoGLOBE_Regions_df_files <- ArchaeoGLOBE_Regions_df$files[ArchaeoGLOBE_Regions_df$files$filename == 'ArchaeGLOBE_Regions.zip', ]

# read into local dir
walk(ArchaeoGLOBE_Regions_df_files$label,
    ~get_file(.x, ArchaeoGLOBE_Regions_DOI) %>% 
      writeBin(paste0('data/raw-data/', .x)))

unzip('data/raw-data/ArchaeGLOBE_Regions.zip', 
      overwrite = TRUE,
      exdir = 'data/raw-data/ArchaeGLOBE_Regions')

# read into the current environment, and simplify the polygons for faster plotting
regions_unsimplified <- 
  st_read('data/raw-data/ArchaeGLOBE_Regions/ArchaeGLOBE_Regions.shp', 
          quiet = TRUE)
regions <- rmapshaper::ms_simplify(regions_unsimplified)
```

## Exploratory plots

Before running any analyses, let's look at the data. How many responses do we have per region?

```{r}
response_counts <- archaeoglobe %>% 
  group_by(REGION_ID) %>%
  count 
```

```{r, fig.width = 6.5, fig.height = 8, echo = FALSE}
a <- regions %>%
  left_join(response_counts, by = c('Archaeo_ID' = 'REGION_ID')) %>%
  mutate(region_label = replace(Archaeo_RG, !(Archaeo_RG %in% c('Hawaii','Polynesia','Micronesia','Melanesia')), NA)) %>%
  ggplot() +
  geom_sf(aes(fill = n), size = .3, color = 'white') +
  scale_fill_viridis_c(name = 'Count', guide = 'legend') +
  geom_sf_text(aes(label = region_label), nudge_x = 500000, size = 2.5, hjust = 0) +
  labs(title = 'Total responses per region', subtitle = '', x = '', y='') +
  theme_minimal()

b <- ggplot(response_counts, aes(n)) +
  geom_bar() +
  theme_bw() +
  labs(x = 'Responses per region', y = 'Number of regions')

a + b + plot_layout(ncol = 1)
ggsave('figures/response_distribution.png', width = 6.5, height = 8)
```

Here is the cumulative summary of regions per land-use category based on consensus assessments (Common > 1% to 20% regional  land area; Widespread > 20% regional land area). 

```{r}

cumsum_landuse_regions <- 
consensus %>% 
  select(Region, FHG_10KBP:URBAN_1850CE) %>% 
  gather(variable, value, - Region) %>% 
  filter(value %in% c("Widespread", "Common", "Split", "Present")) %>% 
  separate(variable, into = c("land_use_category", "years_BP"), sep = "_") %>% 
  mutate(land_use_category = ifelse(str_detect(land_use_category, "AGR"),
                                    str_replace_all(land_use_category , 
                                                    "AGR", 
                                                    "AG"),
                                    land_use_category)) %>% 
  mutate(land_use_category = case_when(
    land_use_category == "FHG"   ~  "Foraging",
    land_use_category == "EXAG"  ~  "Extensive Agriculture",
    land_use_category == "INAG"  ~  "Intensive Agriculture",
    land_use_category == "PAS"   ~  "Pastoralism",
    land_use_category == "URBAN" ~  "Urban Centers") 
    ) %>% 
  mutate(years_BP = ifelse(str_detect(tolower(years_BP), "kbp"), 
                           -parse_number(years_BP) * 1000,
                           ifelse(str_detect(tolower(years_BP), "ce"), 
                           parse_number(years_BP),
                           -parse_number(years_BP)))) %>% 
  unite(land_use_category_consensus_assessments, 
        c('land_use_category', 
          'value'),
        sep = " ") %>%
  complete(land_use_category_consensus_assessments,
           nesting(years_BP)) %>% 
  group_by(land_use_category_consensus_assessments, 
           years_BP) %>% 
  summarise(n = n()) %>% 
  mutate(perc = n / sum(n) * 100)  %>% 
  ungroup() %>% 
  # make an ordered factor for nice plotting
  mutate(land_use_category_consensus_assessments = 
           fct_relevel(land_use_category_consensus_assessments, 
                       c(
                         "Foraging Common",
                         "Foraging Widespread",
                         "Extensive Agriculture Common",
                         "Extensive Agriculture Widespread",
                         "Intensive Agriculture Common",
                         "Intensive Agriculture Widespread",
                         "Pastoralism Common",
                         "Pastoralism Widespread",
                         "Urban Centers Present",
                         "Urban Centers Split"
                         )))
  
  
  
# make years label and breaks
years_label <- unique(ifelse(cumsum_landuse_regions$years_BP < 0,
                      str_glue('{-cumsum_landuse_regions$years_BP} BP'),
                      str_glue('{cumsum_landuse_regions$years_BP} CE')
                      ))

years_breaks <- unique(cumsum_landuse_regions$years_BP)

# draw the plot
ggplot(
  cumsum_landuse_regions,
  aes(years_BP,
      perc,
      fill = land_use_category_consensus_assessments)) +
  geom_area(position = 'stack') +
  scale_fill_brewer(palette = "Set3") +
  scale_x_continuous(labels = years_label,
                     breaks = years_breaks) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      hjust = 1,
      vjust = 0.5),
    #  x- and y- offsets from the bottom-left of the plot, ranging from 0 - 1.
    legend.position = c(0.4, 0.85),
    legend.text = element_text(size = 10),
    legend.key.size = unit(0.7, "line")) +
  guides(fill = guide_legend(ncol = 2, 
                             title = "Land-use category based on consensus assessments")) +
  xlab("") +
  ylab("Percent of all regions")

ggsave('figures/cumulative_sum_land_use.png', height = 5, width = 7)
```


## Analysis functions

Define some analysis functions that we'll be using repeatedly in the analysis, so that we don't have to keep copying and pasting the same lines of code.

This function subsets the data to highlight a variable of interest, and converts it from a wide to a long "tidy" format to make analysis and plotting easier.

```{r}
preprocess <- function(prefix, categories){
  archaeoglobe %>% # start with the full ArcheoGlobe data
    # drop columns not related to the variable of interest
    select(c(CONTRIBUTR:LAND_AREA, starts_with(prefix))) %>%
    gather(time, value, starts_with(prefix)) %>% # one value per row
    mutate(time = parse_number(time) * -1, # convert time period labels to years
           value = ordered(value, levels = categories),
           cat_num = as.numeric(value)) %>%
    mutate_if(is.character, as.factor) # convert characters to factors
}
```

This function takes a data frame produced by the above function and fits GAM to the global trend and local deviations for each region, accounting for inter-observer variability. This function takes as arguments a preprocessed data frame containing time slices, regions, contributors, and the ordered categorical response variable transformed to a numeric vector. 

```{r}
cores <- max(parallel::detectCores() / 2, 1) # physical cores for parallelization
cl <- parallel::makeCluster(cores)

fit_gam <- function(x, n_cats){
  bam(cat_num ~ 
        # this spline is for the global trend
        s(time, bs = 'cr', m = 2) + 
        # region-specific trends. bs = 'ts' and m = 1 
        # help penalize deviation from the global model
        s(time, by = REGION_LAB, bs = 'cs', m = 1) + 
        # add back in region-specific intercepts
        REGION_LAB  +
        # model contributor as a random effect
        s(CONTRIBUTR, bs = 're'),
      data = x, # data frame to analyize
      family = ocat(R = n_cats), # ordered categorical with n levels
      # final 3 arguments just speed up the model fitting
      method = 'fREML',
      discrete = TRUE,
      cluster = cl)
}
```

This function extracts the estimated trends for each region, incorporating the global and regional splines as well as the region and contributor specific intercepts. Then it clusters these trends into 6 discrete clusters.

```{r}
extract_trends <-function(mod, n_clusters = 6){
  set.seed(1000) # set seed for reproducability of clusters
  archaeoglobe %>% # create dummy data for prediction in the following lines
    select(REGION_LAB) %>%
    group_by(REGION_LAB) %>%
    slice(1) %>%
    slice(rep(1:n(), each = 198)) %>%
    ungroup %>%
    mutate(time = rep_len(seq(-10000, -150, 50), n()),
           CONTRIBUTR = 'CYRBU') %>% # select an arbitrary contributor
    mutate(preds = predict(mod, .)) %>% # estimate trend lines
    mutate(preds = plogis(preds)) %>% # transform responses to [0,1] scale
    spread(time, preds) %>%
    # next is the actual kmeans clustering code
    mutate(cluster = kmeans(.[,-c(1,2)], n_clusters, iter.max = 100, nstart = 100)$cluster)
}
```

# Analysis

Now we use the functions defined above on the ArchaeoGlobe data. For convenience, first define a data frame that lists the prefixes of the variables we are interested in (e.g. "EXP" for expertise) and the levels of the ordered factors associated with each variable. This will make it easier to quickly focus on a specific variable. The `tribble` command is simply a way to make a data frame by row rather than column, which makes the code easier to read.

```{r}
response_levels <- tribble(
  ~prefix, ~categories,
  'EXP', c('None', 'Low', 'High'),                  # Expertise
  'DQ', c('Unknown', 'Low', 'Moderate', 'Good'),    # Data Quality
  'HUNT', c('none', 'minimal (<1%)', 'common (1-20%)', 'widespread (>20%)'), 
  'EXAG', c('none', 'minimal (<1%)', 'common (1-20%)', 'widespread (>20%)'),
  'INAG', c('none', 'minimal (<1%)', 'common (1-20%)', 'widespread (>20%)'),
  'PAST', c('none', 'minimal (<1%)', 'common (1-20%)', 'widespread (>20%)'),
  'URBN', c('Absent', 'Present')
)
```

Now map each of the above functions to each variable. This allows us to run the analysis for all variables of interest in a single step, and save all the outputs in a tibble format for easy plotting. If you're running this for the first time, it should take about 40 minutes to run on a Intel NUC with a 5th-gen Intel Core i7-5557U processor and 16gb of RAM running Linux.

```{r echo = T}
# A simple type of caching...
# Do we want to run the modelling code, or
# load a previously saved result from disk, or
# download a previously saved result from a repository?
# default is not run, then check if there is a saved file, and use that, or download

# devtools::install_github('centerforopenscience/osfr')
library(osfr)

rerun_time_consuming_analysis <- FALSE # FALSE means do not run the modelling code when knitting

if(rerun_time_consuming_analysis) {
  message("running the modelling code, this may take 30-50 min...")
  # go to the next chunk of code
} else {
  # check if there is a local file and if so, load it
  if(file.exists('data/derived-data/trend_dat.rda')) {
    # the file exists on the local disk, so just read it in
    message("Loading previously saved model results from disk...")
    trend_dat <- readRDS('data/derived-data/trend_dat.rda')
  } else {
  # we don't want to run the modelling code, and the result don't exist locally,
  # so download
  message("Downloading previously saved model results, takes 2-3 min...")
  trend_dat <- osf_retrieve_file("kcr2e") %>% osf_download('data/derived-data/trend_dat.rda')
  message("Loading the data downloaded from osf.io...")
  trend_dat <- readRDS('data/derived-data/trend_dat.rda')
  writeLines(paste0('trend_dat.rda downloded from https://osf.io/kcr2e/ on ', Sys.Date()), con = 'data/derived-data/README.md', sep = '')
  message("Done.")
  } 
}
```

```{r, eval = rerun_time_consuming_analysis}
trend_dat <- response_levels %>%
  mutate(data = map2(prefix, categories, ~preprocess(.x,.y)),
         n_cats = map_dbl(categories, length), 
         mod = map2(data, n_cats, fit_gam),
         trends = map(mod, extract_trends))

# save to disk
saveRDS(trend_dat, 
        file = "data/derived-data/trend_dat.rda")

# write a note to indicate the provenance of this file
this_commit <-  git2r::revparse_single(git2r::repository('.'), "HEAD")
writeLines(paste0('trend_dat generated on ',
                   Sys.Date() , ' from archaeoglobe.Rmd at git commit ',
                    this_commit$sha, ' made by ',
                    this_commit$author$name, ' on ',
                    this_commit$author$when, " with the message '",
                    this_commit$summary, "'"),
           con = 'data/derived-data/README.md', 
           sep = '')
message("Done.")
```

```{r eval = FALSE, echo = FALSE}
# If we have freshly produced model output, we want to 
# upload to osf.io, this needs to be done manually because we need to 
# authenticate and don't want to put our PAT in the Rmd

osf_auth("my-PAT-for-OSF") # Navigate to https://osf.io/settings/tokens/

# upload the model data
osf_retrieve_node("gcfs6") %>% 
  osf_upload(path = "data/derived-data/trend_dat.rda", 
             overwrite = TRUE)

# upload the README
osf_retrieve_node("gcfs6") %>% 
  osf_upload(path = "data/derived-data/README.md", 
             overwrite = TRUE)
```

# Results

## Global Trends

First we plot out the global trends for each land use type. Please refer to the source .rmd file for the plotting code.
 
```{r echo = FALSE}
 g1data <- trend_dat[3,]$mod[[1]] %>%
    mgcv::plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]] 
    
g1 <- ggplot(g1data,
           aes(time / 1000, plogis(fit))) +
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
    scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'Widespread'), limits = c(0,1)) +
    labs(title = 'A', x = 'Thousand years BP', y = 'Hunting')  +
    theme_bw()

g2data <- trend_dat[6,]$mod[[1]] %>%
    mgcv::plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]] 

g2 <- ggplot(g2data,
           aes(time / 1000, plogis(fit)))+
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
      scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'Widespread'), limits = c(0,1)) +
    labs(title = 'B', x = 'Thousand years BP', y = 'Pastoralism') +
    theme_bw()

g3data <- trend_dat[4,]$mod[[1]] %>%
    plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]] 

g3 <- ggplot(g3data,
           aes(time / 1000, plogis(fit)))+
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
      scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'Widespread'), limits = c(0,1)) +
    labs(title = 'C', x = 'Thousand years BP', y = 'Extensive \nAgriculture') +
    theme_bw()
    
g4data <- trend_dat[5,]$mod[[1]] %>%
    plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]] 

g4 <- ggplot(g4data,
           aes(time / 1000, plogis(fit)))+
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
      scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'Widespread'), limits = c(0,1)) +
    labs(title = 'D', x = 'Thousand years BP', y = 'Intensive \nAgriculture') +
    theme_bw()

g5data <- trend_dat[1,]$mod[[1]] %>%
    plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]] 

g5 <- ggplot(g5data,
           aes(time / 1000, plogis(fit)))+
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
      scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'High'), limits = c(0,1)) +
    labs(title = 'E', x = 'Thousand years BP', y = 'Expertise') +
    theme_bw()

g6data <- trend_dat[2,]$mod[[1]] %>%
    plot.gam(select = 0) %>% # selects the global trend
    .[1] %>%
    map(~tibble(time = .$x, fit = c(.$fit), se = .$se)) %>%
    .[[1]]

g6 <- ggplot(g6data,
           aes(time / 1000, plogis(fit)))+
    geom_line() +
    geom_line(aes(y = plogis(fit + 2 * se)), linetype = 2) +
    geom_line(aes(y = plogis(fit - 2 * se)), linetype = 2) +
      scale_x_continuous(breaks = c(-10, -8,-6,-4,-2,0)) +
    scale_y_continuous(breaks = c(0,1), labels = c('Unknown', 'Good'), limits = c(0,1)) +
    labs(title = 'F', x = 'Thousand years BP', y = 'Data Quality') +
    theme_bw()
```


```{r echo=FALSE}
# global trends
(g1 + g2) / (g3 + g4) / (g5 + g6)
ggsave('figures/global_trends.png', height = 12, width = 12)
```

```{r, echo = FALSE}
plot_trends <- function(x, variable, color_theme = 'PuOr', title = 'Regional land-use trends'){
  p1 <- x$trends[[1]] %>%
    gather(time, value, '-10000':'-150') %>%
    ggplot(aes(as.numeric(time) / 1000, value, 
               group = REGION_LAB, color = as.factor(cluster))) +
    geom_line() +
    scale_color_brewer(palette = color_theme, guide = 'none') +
    scale_x_continuous(breaks = c( -8,-6,-4,-2)) +
    scale_y_continuous(breaks = c(0,1), labels = c('None', 'Widespread')) +
    facet_wrap(~cluster, nrow = 1) +
    labs(x = 'Thousand years BP', y = 'Extent') +
    theme_minimal() +
    theme(strip.background = element_blank(),strip.text.x = element_blank())
  
  p2 <- x$trends[[1]] %>%
    select(REGION_LAB, cluster) %>% # just select the columns of interest
    left_join(select(x$data[[1]], REGION_ID:REGION_LAB)) %>%
    group_by(REGION_ID) %>%
    filter(row_number() == 1) %>%
    left_join(regions, ., by = c('Archaeo_ID' = 'REGION_ID')) %>% # join to the region shapes
    mutate(region_label = replace(Archaeo_RG, !(Archaeo_RG %in% c('Hawaii','Polynesia','Micronesia','Melanesia')), NA)) %>%
    ggplot() + # plot
    geom_sf(aes(fill = as.factor(cluster)), size = .3, color = 'white') +
    geom_sf_text(aes(label = region_label), nudge_x = 500000, size = 2.5, hjust = 0) +
    labs(title = title, subtitle = variable, x = '', y='') +
    scale_fill_brewer(palette = color_theme, guide = 'none') +  
    theme_minimal()

  # the patchwork command that prints the multipanel plots
  p2/p1 + plot_layout(heights = c(5,1), widths = c(4,1))
}
```


## Hunting

The global trend in hunting shows constant high prevalence until around 6,000 years ago, after which there is a smooth decline until the present day when it is very rare. Mapping out the clusters reveals a clear east-west divide, which regions in Afro-eurasia seeing hunting earlier then the global mean, and regions in the Americas and Oceania seeing later peaks in hunting.

```{r echo = FALSE, fig.width = 6.5, fig.cap = 'Regional trends in the areal extent of hunting. (A) Global trend (all regions) with 95% confidence interval. (B) Regional deviations from global trend, clustered via k-means. (C) Map of the local deviations from the global trend, same clusters as in B.'}
trend_dat[3, ] %>%
  mutate(trends = map(trends, ~mutate(.,
                                 cluster = recode_factor(cluster, 
                                 `6` = '1', `5` = '2',`1` = '3', 
                                 `2` = '4', `3` = '5', `4` = '6')))) %>%
  plot_trends('Hunting')
ggsave('figures/trends_hunting.png', height = 8, width = 12)
```

## Extensive Agriculture

The global trends in the prevalence of pastoralism, extensive and intensive agriculture, and urbanism all follow a sigmoidal curve, which means the trend is linear on the scale of the linear predictor (the ordered categorical GAM uses a logit transform as a latent link function). This means that there is a simple increase in the probability of each land use type being prevalent over time.

```{r echo = FALSE, fig.width = 6.5}
trend_dat[4, ] %>%
  mutate(trends = map(trends, ~mutate(.,
                                 cluster = recode_factor(cluster, 
                                 `4` = '1', `2` = '2',`1` = '3', 
                                 `5` = '4', `3` = '5', `6` = '6')))) %>%
plot_trends('Extensive Agriculture')
ggsave('figures/trends_extensive.png', height = 8, width = 12)
```

## Intensive Agriculture

See above.

```{r echo = FALSE, fig.width = 6.5}
trend_dat[5, ] %>%
  mutate(trends = map(trends, ~mutate(.,
                                 cluster = recode_factor(cluster, 
                                `6` = '1', `5` = '2', `3` = '3', 
                                 `1` = '4', `2` = '5', `4` = '6')))) %>%
plot_trends('Intensive Agriculture')
ggsave('figures/trends_intensive.png', height = 8, width = 12)
```

## Pastoralism

See above.

```{r echo = FALSE, fig.width = 6.5}
trend_dat[6, ] %>%
  mutate(trends = map(trends, ~mutate(.,
                                 cluster = recode_factor(cluster, 
                                 `6` = '1', `5` = '2',`3` = '3', 
                                 `1` = '4', `4` = '5', `2` = '6')))) %>%
plot_trends('Pastoralism')
ggsave('figures/trends_pastoralism.png', height = 8, width = 12)
```

## Urbanism

See above.
```{r echo = FALSE, fig.width = 6.5, fig.cap = 'Global and regional trends in the presence of urban centers. (A) Global trend (all regions) with 95% confidence interval. (B) Regional deviations from global trend, clustered via k-means. (C) Map of the local deviations from the global trend, same clusters as in B.'}
trend_dat[7, ] %>%
  mutate(trends = map(trends, ~mutate(.,
                                 cluster = recode_factor(cluster, 
                                 `2` = '1', `6` = '2',`5` = '3', 
                                 `4` = '4', `1` = '5', `3` = '6')))) %>%
plot_trends('Urbanism')
ggsave('figures/trends_urbanism.png', height = 8, width = 12)
```


## Expertise and Data Quality

How does self-professed level of expertise vary in each region over time? The global trend is a roughly linear increase in self-reported expertise from 10ka BP up to 2ka BP, then a falloff continuing to the present day. The present day expertise values are approximately the same as at 10ka BP. This makes sense, as it points to both the increased frequency of preserved archaeological materials with time as well as the reduction in archaeological attention in periods with extensive historical records.

Now we cluster together the local deviations from the global trend using a k-means algorithm. The selection of 6 clusters is somewhat arbitrary, and is made simply based on visual comparisons of different cluster solutions with the goal making the results visually interpretable. The trajectories in these clusters are deviations from the global trend, so a horizontal line would indicate no deviation from the global trend.

The global trend in data quality is more or less the same as the expertise data, with the peak in data quality occurring more recently than for expertise and with a less dramatic falloff leading to the present day. Unlike expertise, which reaches the same values at 10ky BP and present, data quality in the present day remains high in spite of the falloff in the last 2 millennia. Also note the confidence interval for the global trend is generally wider than for the expertise responses.

```{r, echo = FALSE, fig.width = 6.5}
trend_dat[2, ] %>%
  mutate(trends = map(trends, ~mutate(., 
                                  cluster = recode_factor(cluster, 
                                 `3` = '1', `2` = '2',`6` = '3', 
                                 `1` = '4', `5` = '5', `4` = '6')))) %>%
  plot_trends('Data Quality', color_theme = 'RdYlBu', title = 'Archaeological trends') + 
  scale_y_continuous(breaks = c(0, 1), labels = c('Unknown', 'Good')) +
  labs( x = 'Thousand years BP', y = 'Quality')

ggsave('figures/trends_quality.png', height = 8, width = 12)
```

```{r, echo = FALSE, fig.width = 6.5}
trend_dat[1, ] %>%
  mutate(trends = map(trends, 
                      ~mutate(., cluster = recode_factor(cluster, 
                                 `1` = '1', `2` = '2',`3` = '3', 
                                 `5` = '4', `4` = '5', `6` = '6')))) %>%
  plot_trends('Expertise', color_theme = 'RdYlBu', title = 'Archaeological trends') + 
  scale_y_continuous(breaks = c(0,1), labels = c('None', 'High')) +
  labs(x = 'Thousand years BP', y = 'Expertise')

ggsave('figures/trends_exp.png', height = 8, width = 12)
```

## Was the abandonment of widespread foraging more correlated closely with the spread of pastoralism than crop agriculture?

We generated a structural equation model to estimate parameters for a path schematic to test hypotheses about causal relationships between latent variables in the consensus land use data. Structural equation modeling is a multivariate statistical method for analyzing structural relationships that include latent variables (Bollen 1989, Beaujean 2014). The consensus data were recoded to ordinal factors and then input to a diagonally weighted least squares procedure to estimate the structural equation model parameters. To investigate weather the abandonment of widespread foraging was more correlated closely with the spread of pastoralism than crop agriculture, we modeled the consensus responses for foraging, pastoralism and crop agriculture for all regions during the middle and late Holocene. We used the lavaan R package (Rosseel 2012) to fit a model with a Model Fit Test Statistic of 75.199, 18 degrees of freedom. The model fit is good, as indicated by a  Comparative Fit Index (CFI) 0.997 and a Root Mean Square Error of Approximation (RMSEA) of 0.148. The model output shows a regression estimate of -0.674 for foraging and pastoralism, compared to -0.574 for foraging and crop agriculture. The regression coefficient of foraging predicting pastoralism is more negative than foraging predicting crop agriculture, indicating that as foraging was abandoned it was more often replaced by pastoralism than crop agriculture. 

```{r}
consensus_cat <- 
  consensus %>% 
  # convert consensus variables to ordinal factors
  mutate_at(.vars = vars(FHG_10KBP:URBAN_1850CE), 
            .funs = funs(case_when(. == "Widespread" ~ 3, 
                                   . == "Common" ~ 2,  
                                   . == "Minimal" ~  1, 
                                   . == "None" ~ 0))) %>% 
  mutate_at(.vars = vars(FHG_10KBP:URBAN_1850CE), 
            .funs = funs(factor(., ordered = TRUE)))

mod.sem1 <- '
# latent variable definitions
hunt  =~ FHG_6KBP + FHG_4KBP + FHG_3KBP
past  =~ PAS_6KBP + PAS_4KBP + PAS_3KBP
crop  =~ INAG_6KBP + INAG_4KBP + INAG_3KBP
# regressions
past ~ hunt
crop ~ hunt
# residual correlations
FHG_6KBP ~~ PAS_6KBP + INAG_6KBP
FHG_4KBP ~~ PAS_4KBP + INAG_4KBP
FHG_3KBP ~~ PAS_3KBP + INAG_3KBP
'

library(lavaan)
sem.fit <- sem(mod.sem1, 
               data = consensus_cat[,-c(1:2)])
# inspect the summary
summary(sem.fit,
        fit.measures=TRUE, 
        standardized = TRUE)

modificationindices(sem.fit, sort = TRUE)

# look at the diagram:
library(lavaanPlot)
lavaanPlot(name="", 
           model=sem.fit, 
           coefs=TRUE, 
           covs=TRUE, 
           sig=1.00) 


```

Structural equation modelling shows that a causal inference of abandonment of widespread foraging correlating more closely with the spread of pastoralism than crop agriculture is consistent with the data.

Bollen, K. A. (1989). Structural Equations with Latent Variables (Wiley Series in Probability and Statistics, Canada

Beaujean, A. A. (2014). Latent variable modeling using R: A step-by-step guide. Routledge.

Rosseel, Y. (2012). Lavaan: An R package for structural equation modeling and more. Version 0.5â€“12 (BETA). Journal of statistical software, 48(2), 1-36.
